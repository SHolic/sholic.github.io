<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="SHolic的博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="SHolic的博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sholic">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>SHolic的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">SHolic的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/DL-03-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/DL-03-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">DL-03-激活函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:26:39 / 修改时间：10:27:15" itemprop="dateCreated datePublished" datetime="2020-09-16T10:26:39+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="梯度问题"><a href="#梯度问题" class="headerlink" title="梯度问题"></a>梯度问题</h2><p>以sigmoid函数作激活函数为例</p>
<script type="math/tex; mode=display">
sigmoid(x)=\sigma=\frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">
\sigma'(x)=\sigma(x)(1-\sigma(x))=\frac{e^{-x}}{(e^{-x}+1)^2}</script><p><img src="DLimg\activate1.jpeg" alt="activate1" style="zoom:50%;" /><img src="DLimg\activate2.jpeg" alt="activate2" style="zoom:50%;" /></p>
<p>以损失函数对隐藏层的偏置项参数$b$的变化为例</p>
<script type="math/tex; mode=display">
\delta b^l=(a^L-y)y\sigma'(z^L)\cdot w^L\sigma'(z^{L-1})\cdot w^{L-1}\sigma'(z^{L-2})\dots w^{l+1}\sigma'(z^l)</script><p><strong>梯度消失</strong></p>
<ul>
<li>如$\sigma’(x)$图像，$x$越大，$\sigma’(x)$接近于0，累积乘积后，导致$\delta b^l$接近于0，也就是$b$梯度调整过少</li>
<li>如$\delta b^l$公式，当$0&lt;w&lt;1$，累积乘积后，导致$\delta b^l$接近于0，也就是$b$梯度调整过少</li>
</ul>
<p><strong>梯度爆炸</strong></p>
<ul>
<li>如$\delta b^l$公式，当$w&gt;1$，累积乘积后，导致$\delta b^l$快速增长，也就是$b$梯度调整过多</li>
</ul>
<p><strong>梯度死亡</strong></p>
<ul>
<li>某些激活函数（ReLU）在一定区间内导数为0，导致之后的浅层网络$\delta b^l$为0，也就是$b$梯度无调整</li>
</ul>
<p>如果不同层的学习速度不同，那么这个问题还会变得更加严重。层以不同的速度学习，前面几层总是会根据学习率而变得更差。 </p>
<p><img src="DLimg\activate3.jpeg" alt="activate3"></p>
<p>在这个示例中，隐藏层 4 的学习速度最快，因为其成本函数仅取决于连接到隐藏层 4 的权重变化。我们看看隐藏层 1；这里的成本函数取决于连接隐藏层 1 与隐藏层 2、3、4 的权重变化。</p>
<h2 id="从ReLU到GELU"><a href="#从ReLU到GELU" class="headerlink" title="从ReLU到GELU"></a>从ReLU到GELU</h2><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><script type="math/tex; mode=display">
ReLU(x)=\max(0,x)</script><script type="math/tex; mode=display">
ReLU'(x)=\begin{cases}
1&x>0\\
0&x\le0\\
\end{cases}</script><p><img src="DLimg\activate4.jpeg" alt="activate4" style="zoom:50%;" /><img src="DLimg\activate5.jpeg" alt="activate5" style="zoom:50%;" /></p>
<p>优点：</p>
<ul>
<li>相比于sigmoid，由于稀疏性，时间和空间复杂度更低；</li>
<li>不涉及成本更高的指数运算；</li>
<li>能避免梯度消失问题。</li>
</ul>
<p>缺点：</p>
<ul>
<li>引入了死亡ReLU问题，即网络的大部分分量都永远不会更新。但这有时候也是一个优势；</li>
<li>ReLU不能避免梯度爆炸问题。</li>
</ul>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><script type="math/tex; mode=display">
ELU(x)=\begin{cases}
x&x>0\\
\alpha(e^x-1)&x\le0
\end{cases}</script><script type="math/tex; mode=display">
ELU'(x)=\begin{cases}
1&x>0\\
\alpha e^x&x\le0
\end{cases}</script><p>假设$\alpha=0.2$</p>
<p><img src="DLimg\activate6.jpeg" alt="activate6" style="zoom:50%;" /><img src="DLimg\activate7.jpeg" alt="activate7" style="zoom:50%;" /></p>
<p>优点：</p>
<ul>
<li>能避免死亡ReLU问题；</li>
<li>能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化；</li>
<li>在计算梯度时能得到激活，而不是让它们等于 0。</li>
</ul>
<p>缺点：</p>
<ul>
<li>由于包含指数运算，所以计算时间更长；</li>
<li>无法避免梯度爆炸问题；</li>
<li>神经网络不学习$\alpha$值。</li>
</ul>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3><script type="math/tex; mode=display">
LReLU(x)=\begin{cases}
x&x>0\\
\alpha x&x\le0
\end{cases}</script><script type="math/tex; mode=display">
LReLU'(x)=\begin{cases}
1&x>0\\
\alpha&x\le0
\end{cases}</script><p>假设$\alpha=0.2$</p>
<p><img src="DLimg\activate8.jpeg" alt="activate8" style="zoom:50%;" /><img src="DLimg\acitvate9.jpeg" alt="acitvate9" style="zoom:50%;" /></p>
<p>优点：</p>
<ul>
<li>类似ELU，Leaky ReLU也能避免死亡ReLU问题，因为其在计算导数时允许较小的梯度；</li>
<li>由于不包含指数运算，所以计算速度比ELU快。</li>
</ul>
<p>缺点：</p>
<ul>
<li>无法避免梯度爆炸问题；</li>
<li>神经网络不学习 α 值；</li>
<li>在微分时，两部分都是线性的；</li>
<li>而ELU的一部分是线性的，一部分是非线性的。</li>
</ul>
<h3 id="SELU"><a href="#SELU" class="headerlink" title="SELU"></a>SELU</h3><script type="math/tex; mode=display">
SELU(x)=\lambda\begin{cases}
x&x>0\\
\alpha(e^x-1)&x\le0
\end{cases}</script><script type="math/tex; mode=display">
SELU'(x)=\lambda\begin{cases}
1&x>0\\
\alpha e^x&x\le0
\end{cases}</script><p>其中：</p>
<script type="math/tex; mode=display">
\alpha\approx1.6732632423543772848170429916717</script><script type="math/tex; mode=display">
\lambda\approx1.0507009873554804934193349852946</script><p>SELU激活能够对神经网络进行自归一化，网络的组件（权重、偏置和激活）的均值为0，标准差为1</p>
<p><img src="DLimg\activate10.jpeg" alt="activate10" style="zoom:50%;" /><img src="DLimg\activate11.jpeg" alt="activate11" style="zoom:50%;" /></p>
<p>SELU的输出是归一化的，这可称为内部归一化，因此事实上其所有输出都是均值为0且标准差为1。这不同于外部归一化——会用到批归一化或其它方法。</p>
<p>实现SNN（自归一化神经网络）需要：</p>
<ul>
<li>负值和正值，以便控制均值；</li>
<li>饱和区域（导数趋近于零），以便抑制更低层中较大的方差；</li>
<li>大于1的斜率，以便在更低层中的方差过小时增大方差；</li>
<li>连续曲线。</li>
</ul>
<p>后者能确保一个固定点，其中方差抑制可通过方差增大来获得均衡。我们能通过乘上指数线性单元（ELU）来满足激活函数的这些性质，而且$\lambda&gt;1$能够确保正值净输入的斜率大于 1。 </p>
<p>优点：</p>
<ul>
<li>内部归一化的速度比外部归一化快，这意味着网络能更快收敛；</li>
<li>不可能出现梯度消失或爆炸问题，见 SELU 论文附录的定理 2 和 3。</li>
</ul>
<p>缺点：</p>
<ul>
<li>这个激活函数相对较新——需要更多论文比较性地探索其在 CNN 和 RNN 等架构中应用。</li>
<li>这里有一篇使用 SELU 的 CNN 论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.01338.pdf">https://arxiv.org/pdf/1905.01338.pdf</a> </li>
</ul>
<h3 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h3><script type="math/tex; mode=display">
GELU(x)=0.5x(1+\tanh(\sqrt{2/\pi}(x+0.044715x^3)))</script><script type="math/tex; mode=display">
\begin{align}
GELU'(x)&=0.5\tanh(0.0356774x^3+0.797885x)\\&+(0.0535161x^3+0.398942x)\\&sech^2(0.0356774x^3+0.797885x)+0.5
\end{align}</script><p><img src="DLimg\activate12.jpeg" alt="activate12" style="zoom:50%;" /><img src="DLimg\activate13.jpeg" alt="activate13" style="zoom:50%;" /></p>
<p>优点：</p>
<ul>
<li>似乎是 NLP 领域的当前最佳；</li>
<li>尤其在Transformer模型中表现最好；</li>
<li>能避免梯度消失问题。</li>
</ul>
<p>缺点：</p>
<ul>
<li>尽管是 2016 年提出的，但在实际应用中还是一个相当新颖的激活函数。</li>
</ul>
<h2 id="其他激活函数"><a href="#其他激活函数" class="headerlink" title="其他激活函数"></a>其他激活函数</h2><p><img src="DLimg\activate14.gif" alt="activate14"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/DL-02-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/DL-02-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">DL-02-优化算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:25:32 / 修改时间：10:26:14" itemprop="dateCreated datePublished" datetime="2020-09-16T10:25:32+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>每一条梯度是个参数维的向量，并且经历一遍网络结构</p>
<h2 id="方向导数和梯度"><a href="#方向导数和梯度" class="headerlink" title="方向导数和梯度"></a>方向导数和梯度</h2><p><strong>方向导数</strong>指的是函数$z=f(x,y)$在某一点PP沿某一方向的变化率，其表示形式为$\frac{\partial f}{\partial l}$ 。</p>
<p>设函数$z=f(x,y)$在点$P(x,y)$的某一邻域$U(p)$内有定义。自点$P$处引射线$l$，设射线$l$和$X$轴正向的夹角为$\theta$，并且假定射线$l$与函数$z=f(x,y)$的交点为$P′(x+\Delta x,y+\Delta y)$。则函数在$P,P′$的增量为$f(x+Δx,y+Δy)−f(x,y)$，两点之间的距离为$\rho=\sqrt{(\Delta x)^2+(\Delta y)^2}$，当$P′$沿着$l$趋近于$P$时，如果函数增量和两点距离的比值的极限存在，则称这个极限为函数$f(x,y)$在点$P$沿方向$l$的方向导数，记为$\frac{\partial f}{\partial l}$ ，即： </p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial f}{\partial l}&=\lim_{\rho\to0}\frac{f(x+\Delta x,y+\Delta y)-f(x,y)}{\rho}\\
&=\frac{\partial f}{\partial x}\cdot\frac{\Delta x}{\rho}+\frac{\partial f}{\partial y}\cdot\frac{\Delta y}{\rho}+\frac{o(\rho)}{\rho}\\
&=\frac{\partial f}{\partial x}\cdot\cos\theta+\frac{\partial f}{\partial y}\cdot\sin\theta+\frac{o(\rho)}{\rho}\\
&=f_x\cdot\cos\theta+f_y\cdot\sin\theta
\end{align}</script><p> <strong>梯度</strong>是和方向导数相关的一个概念，是一个向量：</p>
<script type="math/tex; mode=display">
\nabla f = gradf(x,y)=\frac{\partial f}{\partial x}i+\frac{\partial f}{\partial y}j</script><p> 设，向量$e=\cos\theta i+\sin\theta j$是与$l$同向的单位向量，则有方向导数的计算公式可知：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial f}{\partial l}&=\frac{\partial f}{\partial x}\cos\theta+\frac{\partial f}{\partial y}\sin\theta\\
&=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}][\cos\theta,\sin\theta]^T\\
&=\nabla f\cdot e\\
&=|\nabla f|\cos<\nabla f,e>\\
&=\sqrt{f_x^2+f^2_y}\cos<\nabla f,e>\\
\end{align}</script><p>一个函数在某点沿着梯度的方向增长最快，而逆着梯度的方向则减小最快。</p>
<h2 id="梯度下降的变体"><a href="#梯度下降的变体" class="headerlink" title="梯度下降的变体"></a>梯度下降的变体</h2><p>$i=1,2,\dots,m$为样本数。</p>
<h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p>批量梯度下降是最原始的形式，它指的是每一次迭代时使用所有样本的数据进行梯度更新</p>
<p>损失函数：</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^i)-y^i)^2</script><p>求导：</p>
<script type="math/tex; mode=display">
\frac{\partial J(\theta_0,\theta_1)}{\partial\theta_j}=\frac{1}{m}\sum^m_{i=1}(h_\theta(x^i)-y^i)x_j^i</script><p>更新参数：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^i)-y^i)x^i_j</script><ul>
<li>优点：每次更是时使用了全部的样本数据，能更准确地朝向极值所在的方向</li>
<li>缺点：当样本很多时，每次都是用所有的样本，训练时每轮的计算量会很大</li>
</ul>
<h3 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h3><p>不同于批量梯度下降，随机梯度下降，每次只用要给样本进行梯度更新，这样训练的速度快上不少</p>
<p>损失函数：</p>
<script type="math/tex; mode=display">
J^i(\theta_0,\theta_1)=\frac{1}{2}(h_\theta(x^i)-y^i)^2</script><p>求导：</p>
<script type="math/tex; mode=display">
\frac{\partial J^i(\theta_0,\theta_1)}{\partial\theta_j}=(h_\theta(x^i)-y^i)x_j^i</script><p>更新参数：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha(h_\theta(x^i)-y^i)x^i_j</script><p>随机梯度下降每次只是用一个样本，其训练速度快。 但是在更新参数的时候，由于每次只有一个样本，并不能代表全部的训练样本，在训练的过程中SGD会一直的波动，这就使得要收敛某个最小值较为困难。</p>
<h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用$n$个小批量训练样本</p>
<p>损失函数：</p>
<script type="math/tex; mode=display">
J^{i:i+n}(\theta_0,\theta_1)=\frac{1}{2n}\sum^n_{i=1}(h_\theta(x^i)-y^i)^2</script><p>求导：</p>
<script type="math/tex; mode=display">
\frac{\partial J^{i:i+n}(\theta_0,\theta_1)}{\partial\theta_j}=\frac{1}{n}\sum^n_{i=1}(h_\theta(x^i)-y^i)x_j^i</script><p>更新参数：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha\frac{1}{n}\sum^n_{i=1}(h_\theta(x^i)-y^i)x^i_j</script><ul>
<li>优点：算是对上面两种方法均衡，即有SGD训练快的优点，每次更新参数时使用多个样本，在训练的过程中较为稳定</li>
<li>缺点：batch_size是一个超参数，需要手动的指定。过大和过小的batch_size会带来一定的问题</li>
</ul>
<h2 id="梯度下降的进化"><a href="#梯度下降的进化" class="headerlink" title="梯度下降的进化"></a>梯度下降的进化</h2><p>梯度下降的难点：</p>
<ul>
<li>学习率的设置</li>
<li>极小值点，鞍点</li>
</ul>
<p><img src="DLimg\gradient1.jpg" alt="gradient1"></p>
<p><strong>指数加权平均</strong></p>
<p>指数加权平均，也称为指数加权移动平均，是常用的一种序列数据的处理方法。设在$t$时刻数据的观测值是$\theta_t$，在$t$时刻的移动平均值为$v_t$，则有：</p>
<script type="math/tex; mode=display">
v_t=\beta v_{t-1}+(1-\beta)\theta_t</script><p>其中，$\beta v_{t−1}$是上一时刻的移动平均值，也是一个历史的积累量。通常设$v_0=0$，$\beta$是一个参数，其值在$(0,1)$之间。移动平均值实际是按比例合并历史量与当前观测量，将上述递推公式展开：</p>
<script type="math/tex; mode=display">
\begin{align}
&v_0=0\\
&v_1=\beta v_0+(1-\beta)\theta_1\\
&v_2=\beta v_1+(1-\beta)\theta_2=\beta(\beta v_0+\theta_1)+(1-\beta)\theta_2\\
&\vdots\\
&v_t=\beta v_{t-1}+(1-\beta)\theta_t=\sum^t_{i=1}\beta^{t-i}(1-\beta)\theta_t
\end{align}</script><p>展开后可以发现，在计算某时刻的$v_t$时，其各个时刻观测值θtθt的权值是呈指数衰减的，离当前时刻$t$越近的$\theta_t$，其权值越大，也就是说<strong>距离当前时刻越近的观测值对求得移动平均值的影响越大，这样得到的平均值的会比较平稳</strong>。由于权重指数衰减，所以移动平均数只是计算比较相近时刻数据的加权平均数，一般认为这个时刻的范围为$\frac{1}{1−β}$，例如$\beta=0.9$，可以认为是使用距离当前时刻之前10时刻内的$\theta_t$的观测值，再往前由于权重值过小，影响较小。 </p>
<h3 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h3><p>动量梯度下降算法基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。 </p>
<script type="math/tex; mode=display">
v_t=\gamma v_{t-1}+\eta\nabla J(\theta)_t</script><script type="math/tex; mode=display">
\theta=\theta-v_t</script><p>在进行参数更新时，使用当前的$v_t$移动平均值来代替当前的梯度，进行参数更新。所谓的动量，也就是近几次的梯度加权移动平均。例如，通常有$\beta=0.9$，也就是当前时刻最近的10次梯度做加权平均，然后用次平均值更新参数。</p>
<p><img src="DLimg\gradient2.jpg" alt="gradient2"></p>
<p>原始的梯度下降算法，会在纵轴上不断的摆动，这种波动就就减慢了梯度下降的速度。理想情况是，在纵轴上希望学习的慢一点，而在横轴上则要学习的快一点，尽快的达到最小值。  </p>
<p>引入动量后，每次使用一段时间的梯度的平均值，这样不同方向的梯度就会相互抵消，从而减缓纵轴方向的波动。而在横轴方向，由于每次梯度的方向都指向同一个位置，引入动量后，其平均后的均值仍然指向同一个方向，并不会影响其下降的速度。</p>
<h3 id="Nesterov加速梯度下降NAG"><a href="#Nesterov加速梯度下降NAG" class="headerlink" title="Nesterov加速梯度下降NAG"></a>Nesterov加速梯度下降NAG</h3><p>球从山上滚下的时候，盲目地沿着斜率方向，往往并不能令人满意。我们希望有一个智能的球，这个球能够知道它将要去哪，以至于在重新遇到斜率上升时能够知道减速。</p>
<p>在动量梯度下降中，使用动量项$\gamma v_{t−1}$来更新参数$\theta$，通过计算$\theta−\gamma v_{t−1}$能够大体预测更新后参数所在的位置，也就是参数大致将更新为多少。通过计算关于<strong>参数未来的近似位置的梯度，而不是关于当前的参数的梯度位置</strong> </p>
<script type="math/tex; mode=display">
v_t=\gamma v_{t-1}+\eta\nabla J(\theta-\gamma v_{t-1})</script><script type="math/tex; mode=display">
\theta=\theta-v_t</script><p><img src="DLimg\gradient3.jpg" alt="gradient3"></p>
<p>动量梯度下降，首先计算当前的梯度项，上图的蓝色小向量；然后加上累积的动量项，得到大蓝色向量，在改方向上前进一步。</p>
<p>NAG则首先在之前累积的动量项（棕色向量)前进一步，计算梯度值，然后做一个修正（绿色的向量）。这个具有预见性的更新防止我们前进得太快，同时增强了算法的响应能力。</p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>AdaGrad的基本思想是对每个变量使用不同的学习率。在最初，学习速率较大，用于快速下降。随着优化过程的进行，对于已经下降很多的变量，则减小学习率；对于没有怎么下降的变量，则仍保持大的学习率。</p>
<p>AdaGrad对每个变量更新时，利用该变量历史积累的梯度来修正其学习速率。这样，已经下降的很多的变量则会有小的学习率，而下降较少的变量则仍然保持较大的学习率。基于这个更新规则，其针对变量$\theta_i$的更新：</p>
<script type="math/tex; mode=display">
\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\sum^t_{\tau=1}(\nabla J(\theta_i)_\tau\odot\nabla J(\theta_i)_\tau)+\epsilon}}\cdot\nabla J(\theta_i)_{t+1}</script><p>其中,$\nabla J(θ_i)_t$表示t时刻变量$\theta_i$的梯度。$\sum^t_{\tau=1}\nabla J(\theta_i)_\tau$就表示变量$\theta_i$历史累积的梯度值 ，用来修正学习率。加上很小的值$\epsilon$是为了防止0的出现。</p>
<p>存在问题：</p>
<ul>
<li>仍然需要手动的设置一个初始的全局学习率</li>
<li>使用变量的历史累积梯度来调整学习率，这就导致其学习率是不断衰减的，训练后期学习速率很小，导致训练过早停止。</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其思路引入了动量（指数加权移动平均数）的方法，引入了超参数$\gamma$,在累积梯度的平方项近似衰减：</p>
<script type="math/tex; mode=display">
s_{t,i}=\gamma s_{t-1,i}+(1-\gamma)\nabla J(\theta_i)_t\odot\nabla J(\theta_i)_t</script><script type="math/tex; mode=display">
\theta_{t,i}=\theta_{t-1,i}-\frac{\eta}{\sqrt{s_{t,i}+\epsilon}}\cdot\nabla J(\theta_i)_t</script><p>其中，$i$表示第$i$个变量，$t$表示$t$时刻更新，$\gamma$是超参数通常取$\gamma=0.9$。$s_{t,i}$表示梯度平方的指数加权移动平均数，用来代替AdaGrad中不断累加的历史梯度，有助于避免学习速率衰减过快的问题。全局的学习率$\eta$一般设置为0.001。</p>
<h3 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h3><p>AdaDelta对AdaGrad进行两方面的改进：</p>
<ul>
<li>学习率衰减过快</li>
<li>全局学习率超参数问题</li>
</ul>
<p>针对学习率衰减过快的问题，其思路和RMSprop一样，不在累积所有的历史梯度，而是引入指数加权平均数，只计算一定时间段的梯度。</p>
<p>为了解决学习率超参数的问题，AdaDelta维护了一个额外的状态变量$\Delta\theta_t$，根据RMSprop公式有：</p>
<script type="math/tex; mode=display">
\Delta\theta_t=-\frac{\eta}{\sqrt{s_{t,i}+\epsilon}}\odot\nabla J(\theta_i)_t</script><p>上述使用的是梯度平方的指数加权移动平均数，在AdaDelta中作者又定义了：每次参数的更新值$\Delta\theta$的平方的指数加权移动平均数：</p>
<script type="math/tex; mode=display">
E(\Delta\theta^2)_t=\gamma E(\Delta\theta^2)_{t-1}+(1-\gamma)\Delta\theta_t^2</script><p>因此每次更新时，更新值的均方根：</p>
<script type="math/tex; mode=display">
RMS(\Delta\theta)_t=\sqrt{E(\Delta\theta^2)_t+\epsilon}</script><p>使用$RMS(\Delta\theta)_{t−1}$来近似更新$t$时刻的学习速率$\eta$，这样可以得到其更新的规则为：</p>
<script type="math/tex; mode=display">
\theta_{t,i}=\theta_{t-1,i}-\frac{RMS(\Delta\theta)_{t-1}}{\sqrt{s_{t,i}+\epsilon}}\odot\nabla J(\theta_i)_t</script><p>设初始的$RMS(\Delta\theta)_0=0$，这样就不用设置默认的学习速率了。 也就是，AdaDelta和RMSprop唯一的区别，就是使用$RMS(\Delta\theta)_{t-1}=0$来代替超参数学习速率。 </p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p> 结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体。 </p>
<p> Adam不只使用梯度平方的指数加权移动平均数$v_t$，还使用了梯度的指数加权移动平均数$m_t$，类似动量。 </p>
<script type="math/tex; mode=display">
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t</script><script type="math/tex; mode=display">
v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2</script><p>可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，需要对其进行放大：</p>
<script type="math/tex; mode=display">
\hat m_t=\frac{m_t}{1-\beta^t_1}</script><script type="math/tex; mode=display">
\hat v_t=\frac{v_t}{1-\beta^t_2}</script><p>更新规则：</p>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat v_t}+\epsilon}\hat m_t</script><p>建议$\beta_1$设置为0.9，$\beta_2$设置为0.999，取$\epsilon=10^{−8}$。 </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><p>移动平均相当于利用了窗口范围内的历史数据</p>
</li>
<li><p>Momentem相对传统梯度下降，移动平均了梯度减少震荡加速下降</p>
</li>
<li>NAG相对传统梯度下降，移动平均了梯度减少震荡加速下降，提前预测了梯度缓解加速过快</li>
<li>AdaGrad相对传统梯度下降，用所有历史梯度调整了学习率</li>
<li>RMSprop相对传统梯度下降，移动平均了历史梯度调整了学习率</li>
<li>AdaDelta相对传统梯度下降，移动平均了历史梯度调整了学习率，移动平均了参数增量替代了学习率</li>
<li>Adam相对传统梯度下降，移动平均了梯度减少震荡加速下降，移动平均了历史梯度调整了学习率</li>
</ul>
<p>各梯度在马鞍面的效果如下：</p>
<p><img src="DLimg\gradient4.gif" alt="gradient4"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/DL-01-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/DL-01-%E5%89%8D%E5%90%91%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD/" class="post-title-link" itemprop="url">DL-01-前向后向传播</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:23:12 / 修改时间：10:24:57" itemprop="dateCreated datePublished" datetime="2020-09-16T10:23:12+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>深度学习中，每一条记录是一串多维特征，每一特征就是一个节点</p>
<h2 id="前向传播过程"><a href="#前向传播过程" class="headerlink" title="前向传播过程"></a>前向传播过程</h2><p><img src="DLimg\bp1.png" alt="bp1"></p>
<p>记$w^l_{jk}$为第$l-1$层第$k$个神经元到第$l$层第$j$个神经元的权重，$b^l_j$为第$l$层第$j$个神经元的偏置，$a^l_j$为第$l$层第$j$个神经元的激活值（激活函数的输出）。 </p>
<p>如上图$w^3_{24}$表示的是第3层第2个神经元和第2层第4个神经元之间连线（可以看成是$w^{32}_{24}$的省略写法）。 </p>
<p>不难看出，$a^l_j$的值取决于上一层神经元的激活：</p>
<script type="math/tex; mode=display">
a^l_j=\sigma(z^l_j)=\sigma(\sum_kw^l_{jk}a^{l-1}_k+b^l_j)</script><p>一般化：</p>
<script type="math/tex; mode=display">
a^l=\sigma(z^l)=\sigma(w^la^{l-1}+b^l)</script><h2 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h2><p><strong>最后一层</strong></p>
<p>最后一层均方误差和作损失函数：</p>
<script type="math/tex; mode=display">
\begin{align}
J(W,b,x,y)&=\frac{1}{2}\|a^L-y\|^2_2\\
&=\frac{1}{2}\|\sigma(w^La^{L-1}+b^L)-y\|^2_2\\
\end{align}</script><p> 根据复合函数链式求导法则，L层参数$W^L,b^L$的梯度容易求得： </p>
<script type="math/tex; mode=display">
\delta w^L=\frac{\partial J(W,b,x,y)}{\partial W^L}=\frac{\partial J(W,b,x,y)}{\partial z^L}\cdot\frac{\partial z^L}{\partial W^L}=\delta^L(a^{L-1})^T</script><script type="math/tex; mode=display">
\delta b^L=\frac{\partial J(W,b,x,y)}{\partial b^L}=\frac{\partial J(W,b,x,y)}{\partial z^L}\cdot\frac{\partial z^L}{\partial b^L}=\delta^L</script><p>其中重叠部分$\delta^L$：</p>
<script type="math/tex; mode=display">
\delta^L=\frac{\partial J(W,b,x,y)}{\partial z^L}=(a^L-y)\odot\sigma'(z^L)</script><p><strong>普通层</strong></p>
<p>普通隐藏层$L-1,L-2,\dots,l,\dots,1$，我们只需要求出损失函数相对$l$层非激活输出$z^l$的导数，再根据前向传播公式$z^l=w^la^{l-1}+b^l$便可以轻易的求得$W^L,b^l$。</p>
<script type="math/tex; mode=display">
\delta w^l=\frac{\partial J(W,b,x,y)}{\partial W^l}=\frac{\partial J(W,b,x,y)}{\partial z^l}\cdot\frac{\partial z^l}{\partial W^l}=\delta^l(a^{l-1})^T</script><script type="math/tex; mode=display">
\delta b^l=\frac{\partial J(W,b,x,y)}{\partial b^l}=\frac{\partial J(W,b,x,y)}{\partial z^l}\cdot\frac{\partial z^l}{\partial b^l}=\delta^l</script><p>其中重叠部分$\delta^l$：</p>
<script type="math/tex; mode=display">
\begin{align}
\delta^l&=\frac{\partial J(W,b,x,y)}{\partial z^l}\\
&=\frac{\partial J(W,b,x,y)}{\partial z^L}\frac{\partial z^L}{\partial z^{L-1}}\frac{\partial z^{L-1}}{\partial z^{L-2}}\dots\frac{\partial z^{l+1}}{\partial z^l}\\
&=\frac{\partial J(W,b,x,y)}{\partial z^{l+1}}\frac{\partial z^{l+1}}{\partial z^l}\\
&=\delta^{l+1}\frac{\partial(w^{l+1}\sigma(z^l)+b^{l+1})}{\partial z^l}\\
&=\delta^{l+1}w^{l+1}\sigma'(z^l)\\
&=(a^L-y)y\sigma'(z^L)\cdot w^L\sigma'(z^{L-1})\cdot w^{L-1}\sigma'(z^{L-2})\dots w^{l+1}\sigma'(z^l)
\end{align}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-20-BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-20-BERT/" class="post-title-link" itemprop="url">NLP-20-BERT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:16:18 / 修改时间：10:17:05" itemprop="dateCreated datePublished" datetime="2020-09-16T10:16:18+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>利用了真双向的Transformer</li>
<li>为了利用双向信息，改进了普通语言模型成为完形填空式的Mask-LM(Mask-Language Model)</li>
<li>利用Next Sentence Prediction任务学习句子级别信息</li>
<li>进一步完善和扩展了GPT中设计的通用任务框架，使得BERT能够支持包括：句子对分类任务、单句子分类任务、阅读理解任务和序列标注任务</li>
</ul>
<h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>BERT是Bidirectional Encoder Representations from Transformers的缩写，BERT是Transformer中的 Encoder。它由许多个Encoder堆叠而成</p>
<p><img src="NLPimg\bert1.jpg" alt="bert1"></p>
<h2 id="预训练语言任务"><a href="#预训练语言任务" class="headerlink" title="预训练语言任务"></a>预训练语言任务</h2><h3 id="词级别语义关系：Masked-Language-Model"><a href="#词级别语义关系：Masked-Language-Model" class="headerlink" title="词级别语义关系：Masked Language Model"></a>词级别语义关系：Masked Language Model</h3><p>步骤：</p>
<ul>
<li>输入序列依然和普通Transformer保持一致，只不过把挖掉的一个词用”[MASK]”替换</li>
<li>Transformer的Encoder部分按正常进行</li>
<li>输出层在被挖掉的词位置，接一个分类层做词典大小上的分类问题，得到被mask掉的词概率大小</li>
</ul>
<p>在Masked LM中，我们会把输入的句子中随机将15%的词汇置换为一个特殊的token，叫做[MASK]</p>
<p>其实并不是所有被选中的词汇都会被替换为 [MASK]</p>
<ul>
<li>80%被替换为[MASK]，解决信息泄露问题</li>
<li>10%被替换为随机token，模型的输出向量朝输入层的真实embedding有一个偏移</li>
<li>10%不替换，迫使模型尽量在每一个词上都学习到一个全局语境下的表征，解决一词多义</li>
</ul>
<p>BERT的任务就是猜出这些被置换掉的词汇的什么</p>
<p><img src="NLPimg\bert3.png" alt="bert3"></p>
<p>经过BERT后我们得到一个embedding，再把置换为[MASK]的那个位置输出的embedding通过一个线性分类器，预测这个单词是什么</p>
<h3 id="句子级别语义关系：Next-Sentence-Prediction"><a href="#句子级别语义关系：Next-Sentence-Prediction" class="headerlink" title="句子级别语义关系：Next Sentence Prediction"></a>句子级别语义关系：Next Sentence Prediction</h3><p>在Next Sentence Prediction中，我们给BERT两个句子，让BERT预测这两个句子是不是接在一起的</p>
<p><img src="NLPimg\bert4.png" alt="bert4"></p>
<ul>
<li>[SEP]：特殊的token，代表两个句子的交界处</li>
<li>[CLS]：特殊的token，代表要做分类 </li>
</ul>
<p>我们再把[CLS]输出的向量通过一个线性分类器，让分类器判断这两个句子应不应该接在一起。BERT是Transformer的Encoder，它用到了self-attention机制，可以读到句子的全部信息，所以[CLS]可以放在开头</p>
<p>数据中有50%的情况这两个句子是先后关系，而另外50%的情况下，这两个句子是随机从语料中凑到一起的</p>
<h3 id="Multi-task训练"><a href="#Multi-task训练" class="headerlink" title="Multi-task训练"></a>Multi-task训练</h3><p>BERT的损失函数由两部分组成</p>
<ul>
<li>第一部分是来自于Mask-LM的单词级别的分类任务</li>
<li>另一部分是句子级别的分类任务，通过这两个任务的联合学习</li>
</ul>
<p>具体的损失函数如下：</p>
<script type="math/tex; mode=display">
L(\theta,\theta_1,\theta_2)=L_1(\theta,\theta_1)+L_2(\theta,\theta_2)</script><p>其中$\theta$是BERT中Encoder部分的参数，$\theta_1$是Mask-LM任务中在Encoder上所接的输出层中的参数，$\theta_2$则是句子预测任务中在Encoder上接上的分类器中的参数。</p>
<script type="math/tex; mode=display">
L_1(\theta,\theta_1)=-\sum_{i=1}^M\log p(m=m_i|\theta,\theta_1),m_i\in[1,2,\dots,|V|]</script><script type="math/tex; mode=display">
L_2(\theta,\theta_2)=-\sum_{j=1}^N\log p(n=n_j|\theta,\theta_2),n_j\in[IsNext,NotNext]</script><p>BERT还利用了一系列策略，使得模型更易于训练，比如对于学习率的warm-up策略（和之前提到的ULMFiT以及Transformer中用到的技巧类似），使用的激活函数不再是普通的ReLu，而是GeLu，也是用了dropout等这些比较常见的训练技巧。 </p>
<h2 id="下游任务fine-tuning"><a href="#下游任务fine-tuning" class="headerlink" title="下游任务fine-tuning"></a>下游任务fine-tuning</h2><p>为了适应各种场景，需要通用输入输出层</p>
<h3 id="BERT输入"><a href="#BERT输入" class="headerlink" title="BERT输入"></a>BERT输入</h3><p>在BERT中</p>
<ul>
<li>起始标记都用“[CLS]”来表示，结束标记符用”[SEP]”表示</li>
<li>对于两个句子的输入情况，除了起始标记和结束标记之外，两个句子间通过”[SEP]”来进行区分。</li>
<li>用了两个表示当前是句子A还是句子B的向量来进行表示，对于句子A来说，每一词都会添加一个同样的表示当前句子为句子A的向量，相应的，如果有句子B的话，句子B中的每一个词也都会添加一个表示当前句子为句子B的向量。</li>
<li>为了引入序列中词的位置信息，也用了position embedding</li>
</ul>
<p><img src="NLPimg\bert2.png" alt="bert2"></p>
<p>如上图所处，BERT的输入embedding是token embeddings、segment embeddings和position embedding的和</p>
<ul>
<li>token embedding：词向量</li>
<li>segment embedding：表示句子是第一句还是第二句</li>
<li>position embedding：表示词汇在句子中的位置</li>
</ul>
<h3 id="BERT输出"><a href="#BERT输出" class="headerlink" title="BERT输出"></a>BERT输出</h3><p>BERT主要针对四类任务考虑和设计了一些非常易于移植的输出层</p>
<ul>
<li>单个序列文本分类任务（单句做encoder，第一个输出作分类预测）</li>
<li>两个序列文本分类任务（句子对做encoder，第一个输出作分类预测）</li>
<li>阅读理解任务（只需对每一个位置预测是否为start和是否为end）和序列标注任务(对每一位置多分类预测)</li>
<li>句子或答案选择任务（分解为多个句子对encoder，一起全连接到二分类）</li>
</ul>
<p><img src="NLPimg\bert5.png" alt="bert5"></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul>
<li>特征提取器，只训练全连接<ul>
<li>将预训练模型当做一个特征提取器，直接将预训练模型的输出层去掉，然后使用去掉输出层之后的最后一层输出作为特征输入到我们自己精心设计好的Task-specific模型中去，在训练过程中，作为特征提取器的部分（比如BERT Encoder）的参数是不变的。另外，特征提取器并不一定只能用最后一层的输出向量，也可以使用中间的某些层，甚至也可以借鉴ELMo的做法，在各层之间利用一个softmax来学习各自的权值，或者也可以尝试一些更为复杂的各层组合方式；</li>
</ul>
</li>
<li>预训练模型与全连接一起训练<ul>
<li>将预训练模型整体接入Task-specific模型，继而重新在新的数据集上整体重新训练，当然训练技巧可以有很多种，比如ULMFiT的三角学习率和逐层解冻或者是Transformer的warmup策略（上文都有提到），这些训练技巧非常重要，需要好好把控，否则很容易学崩了，甚至让原有预训练语言模型的优势都被新的finetune抹去了，因此需要实验设计一个比较好的finetune策略。此外，和特征提取器的接入方式类似，预训练模型的接入不一定只能接最后层的输出，可以尝试更复杂的接入方式，比如DenseNet；</li>
</ul>
</li>
<li>训练部分预训练模型和全连接<ul>
<li>和上面两种极端情况相反，或者说综合了上面两种方式的方案，即保留预训练模型的一部分，另外一部分则和Task-specific模型一起finetune，在某些情况下，这可能是个比较合理的选择，比如预训练模型比较深（NLP模型通常来讲比CV模型都要浅很多），以及训练数据不算太多的情况，这个时候一方面要保证预训练模型在大规模语料上曾经学习到的表征，另一方面因为又要做新数据下的迁移，但是数据量比较少，重新finetune整个模型可能不太合适，容易导致模型的鲁棒性不高，那么似乎选择最后的一些层进行选择性的finetune会是比较好的方案。</li>
</ul>
</li>
</ul>
<h2 id="GPT与BERT对比"><a href="#GPT与BERT对比" class="headerlink" title="GPT与BERT对比"></a>GPT与BERT对比</h2><p><img src="NLPimg\bert6.png" alt="bert6"><img src="NLPimg\bert7.png" alt="bert7"></p>
<ul>
<li>GPT因为采用了传统语言模型所以更加适合用于自然语言生成类的任务(NLG)，因为这些任务通常是根据当前信息生成下一刻的信息。而BERT更适合用于自然语言理解任务(NLU) </li>
<li>GPT 采用了Transformer的Decoder，而BERT采用了Transformer的Encoder。GPT使用Decoder中的Mask Multi-Head Attention结构，在使用$[u_1,u_2,\dots,u_{i-1}]$预测单词$u_i$的时候，会将$u_i$之后的单词Mask掉。 </li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-19-GPT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-19-GPT/" class="post-title-link" itemprop="url">NLP-19-GPT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:15:32 / 修改时间：10:16:03" itemprop="dateCreated datePublished" datetime="2020-09-16T10:15:32+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>GPT 使用 Transformer 的 Decoder 结构，并对 Transformer Decoder 进行了一些改动，原本的 Decoder 包含了两个 Multi-Head Attention 结构，GPT 只保留了 Mask Multi-Head Attention</p>
<p><img src="NLPimg\gpt1.png" alt="gpt1"></p>
<h2 id="Mask操作"><a href="#Mask操作" class="headerlink" title="Mask操作"></a>Mask操作</h2><p>Mask 操作是在 Self-Attention 进行 Softmax 之前进行的，具体做法是将要 Mask 的位置用一个无穷小的数替换 -inf，然后再 Softmax，如下图所示。</p>
<p><img src="NLPimg\gpt2.jpeg" alt="gpt2"></p>
<p><img src="NLPimg\gpt3.jpeg" alt="gpt3"></p>
<h2 id="预训练语言任务"><a href="#预训练语言任务" class="headerlink" title="预训练语言任务"></a>预训练语言任务</h2><p>给定句子$U=[u_1,u_2,\dots,u_n]$，GPT训练语言模型时需要最大化下面的似然函数</p>
<script type="math/tex; mode=display">
L_1(U)=\sum_i\log P(u_i|u_{i-k},\dots,u_{i-1};\Theta)</script><p>可以看到GPT是一个单向的模型，GPT的输入用$h_0$表示，$h_0$的计算公式如下</p>
<script type="math/tex; mode=display">
h_0=UW_e+W_p</script><p>$W_p$是单词位置的Embedding，$W_e$是单词的Embedding。用$voc$表示词汇表大小，$pos$表示最长的句子长度，$dim$表示Embedding维度，则$W_p$是一个$pos\times dim$的矩阵，$W_e$是一个$voc\times dim$的矩阵。</p>
<p>得到输入$h_0$之后，需要将$h_0$依次传入GPT的所有Transformer Decoder里，最终得到$h_t$</p>
<script type="math/tex; mode=display">
h_t=transformer\_block(h_{l-1}),l\in[1,t]</script><p>最后得到$h_t$再预测下个单词的概率</p>
<script type="math/tex; mode=display">
P(u)=softmax(h_tW_e^T)</script><p>为了能够高效节省时间和内存的处理如此长的序列，做了一些Memory-Compressed的工作，主要是两方面：</p>
<ul>
<li>一方面是把一个batch内部的序列按长度进行分组，然后分别在每个组内部进行self-attention操作，这样可以避免将一些很短的句子也padding到整个语料的最大长度；</li>
<li><p>另一方面，通过CNN的操作，把K和V压缩到序列长度更小的一个矩阵，同时保持Q不变，这样也能相当程度上减少计算量。</p>
<p><img src="NLPimg\gpt4.jpg" alt="gpt4"></p>
</li>
</ul>
<h2 id="下游任务fine-tuning"><a href="#下游任务fine-tuning" class="headerlink" title="下游任务fine-tuning"></a>下游任务fine-tuning</h2><p>GPT 经过预训练之后，会针对具体的下游任务对模型进行微调。微调的过程采用的是有监督学习，训练样本包括单词序列$[x_1,x_2,\dots,x_m]$和 类标$y$。GPT微调的过程中根据单词序列$[x_1,x2,\dots,x_m]$预测类标$y$</p>
<script type="math/tex; mode=display">
P(y|x^1,\dots,x^m)=softmax(h^m_lW_y)</script><p>$W_y$表示预测输出时的参数，微调时候需要最大化以下函数</p>
<script type="math/tex; mode=display">
L_2=\sum_{x,y}\log P(y|x^1,\dots,x^m)</script><p>GPT在微调的时候也考虑预训练的损失函数，所以最终需要优化的函数为：</p>
<script type="math/tex; mode=display">
L_3=L_2+\lambda L_1</script><p><img src="NLPimg\gpt5.jpg" alt="gpt5"></p>
<ol>
<li>对于分类问题，不用怎么动，加上一个起始和终结符号即可；</li>
<li>对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；</li>
<li>对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；</li>
<li>对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。 </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-18-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-18-Transformer/" class="post-title-link" itemprop="url">NLP-18-Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:14:13 / 修改时间：10:15:08" itemprop="dateCreated datePublished" datetime="2020-09-16T10:14:13+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>RNN系列的一个天生局限就是其时序性，以文本序列为例，我们必须要先计算出t时刻的隐状态信息，才能够继续计算下一时刻的相应信息，这就导致大规模数据下模型整体效率比较低，难以实现计算的并行化。Transformer将Encoder和Decoder部分的RNN系列全部替换为Attention，使模型变成Attention is all you need</p>
<p><img src="NLPimg\transformer1.png" alt="transformer1"> </p>
<h2 id="self-Attention"><a href="#self-Attention" class="headerlink" title="self-Attention"></a>self-Attention</h2><h3 id="框架-1"><a href="#框架-1" class="headerlink" title="框架"></a>框架</h3><p><img src="NLPimg\transformer2.png" alt="transformer3"></p>
<h3 id="Attention一般形式"><a href="#Attention一般形式" class="headerlink" title="Attention一般形式"></a>Attention一般形式</h3><p>Decoder上下文信息$c$与Encoder隐状态$h_j$和Decoder隐状态$s_{i-1}$关系</p>
<script type="math/tex; mode=display">
c_i=\sum_ja_{ij}h_j=\sum_ja(s_{i-1},h_j)h_j</script><ul>
<li>第一类是需要与一个变量集合逐一比较的，我们称为query，简记为$q$。</li>
<li>而那个集合里每一个变量作为第二类变量，是要与$q$进行比较的，称为key，简记为$k$。</li>
<li>最后一类变量是被权重$a_{ij}$对应j位置赋权的，称为value，简记为$v$(这样定义，很容易发现key和value是一一对应的，因为第j个需要和query比较的key，得到的权重要赋予给第j个value)。</li>
</ul>
<p>与上述具体的过程对应起来，就是$q=s_{i-1},k=v=h_j$</p>
<p>用矩阵表示</p>
<script type="math/tex; mode=display">
Q=(q_1,q_2,\dots,q_m)^T,K=(k_1,k_2,\dots,k_n)^T,V=(v_1,v_2,v_n)^T</script><p>其中，m和n是序列长度</p>
<p>Attention一般形式表述：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=(a(Q_1,K)V,a(Q_2,K)V,\dots,a(Q_m,K)V)</script><script type="math/tex; mode=display">
a(Q_1,K)=(softmax(a(q_1,k_1)),softmax(a(q_1,k_2)),\dots,softmax(a(q_1,k_n)))</script><p>用到乘性打分函数，一般形式为：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(QK^T)V</script><p>假设$Q$，$K$中的每一个$d_k$维向量是$q$，$k$。$q$，$k$中的每个分量独立同分布于均值为0，方差为1的随机变量。那么$q\cdot k$就将服从于均值为0，方差为$d_k$的随机变量。这就在作为$d_k$的方差很大的时候，导致对于固定的$q_i$来说，其与每个$k_j$的内积将会差异很大。那么在进行softmax运算的时候，就会呈现要么趋向于0，要么趋向于1的情况，因此导致整体梯度的更新处于一个较小的区域。</p>
<p>引入了缩放因子以消除大方差的影响：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><h3 id="self-Attention-1"><a href="#self-Attention-1" class="headerlink" title="self-Attention"></a>self-Attention</h3><p>Transformer的核心思想就是用Attention来替代原来Encoder和Decoder部分的RNN，先前RNN的主要作用就是将当前时刻输入的词向量信息与前一时刻隐状态进行整合，形成该时刻的隐状态信息，换句话说，就是将词向量信息抽象为可供模型使用的高层次信息。</p>
<p><img src="NLPimg\transformer3.svg" alt="Transformer2"></p>
<p>self-Attention(自注意力机制) ，优点是所有待处理的向量在矩阵运算下可以很好的并行处理，加快了以前Encoder和Decoder的效率。</p>
<h3 id="Multi-Headed-Attention"><a href="#Multi-Headed-Attention" class="headerlink" title="Multi-Headed Attention"></a>Multi-Headed Attention</h3><p>多数情况下，单一的self-Attention难以捕获序列信息的多样性。因此考虑多个相同操作的self-Attention平行的提取每个词语的信息，然后再将多个Attention的结果拼接起来，用于后续层次的操作。这点类似于卷积神经网络中多个卷积核同时作用一个矩阵对象的想法，都是试图取实现信息不同角度的多样化采集。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional-Encoding"></a>Positional-Encoding</h2><p>在RNN当中可以很好的保留词与词之间的序列顺序，现在通过self-Attention并行化以后，这个信息难以被描述出来。</p>
<p>引入位置向量$PE(pos)$，并与输入的词向量进行求和，之后输入对应的下一层，这就要求位置向量应该与词向量具有相同的维度$d_{model}$ ,并在奇偶性不同的分量位置上设计了不同的取值，如下：</p>
<script type="math/tex; mode=display">
PE(pos,2i)=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})，偶数位分量</script><script type="math/tex; mode=display">
PE(pos,2i+1)=\cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})，奇数位份量</script><p>其中，pos为当前词语在句子中的位置(1,2,3,4….)，i为位置向量的分量位置</p>
<p>位置的偏移角度看$PE(pos+k)$</p>
<script type="math/tex; mode=display">
\begin{align}PE(pos+k)
&=\sin(\frac{pos+k}{10000^{\frac{2i}{d_{model}}}})\\
&=\sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\cos(\frac{k}{10000^{\frac{2i}{d_{model}}}})+cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\sin(\frac{k}{10000^{\frac{2i}{d_{model}}}})\\
&=PE(pos,2i)PE(k,2i+1)+PE(pos,2i+1)PE(k,2i)
\end{align}</script><p>在pos位置偏移k个位置以后的位置向量可以表示为pos位置向量的线性组合，使得位置的相对信息有了一定体现。</p>
<h2 id="残差边"><a href="#残差边" class="headerlink" title="残差边"></a>残差边</h2><p>残差边（short-cut）在图像的resnet中已经被广泛应用了，它的出现是为了防止网络层数的加深导致网络内的参数退化。其想法是将低层的特征跃过一些网络层直接送进高层网络，这就保证了网络的最差情况也能学到那个低层特征本身。 </p>
<p>原因是过去的网络是计算$l$层输入$x(l)$的线性与非线性的变换$z^{(l+1)}=\sigma(wx^{(l)}+b)$，而残差边的思想是将低层的（比如$l-1$层$x^{(l-1)}$）特征直接送入$l$层参与运算$z^{(l+1)}=\sigma(wx^{(l)}+b+x^{(l-1)})$,这样一来，即使参数w，b都退化为0，也还能留下低层特征的信息送到高层，不至于让信息在深层网络传递的过程中丢失过多。</p>
<h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer-Normalization"></a>Layer-Normalization</h2><p>防止数据的偏差越来越大导致的梯度消失或者梯度爆炸 </p>
<p>BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示： </p>
<p><img src="NLPimg\transformer3.png" alt="transformer3"></p>
<p>右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下：</p>
<script type="math/tex; mode=display">
BN(x_i)=\alpha\times\frac{x_i-\mu_b}{\sqrt{\sigma^2_B+\epsilon}}+\beta</script><p>LN也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差：</p>
<p><img src="NLPimg\transformer4.png" alt="transformer4"></p>
<p>下面看一下 LN 的公式： </p>
<script type="math/tex; mode=display">
LN(x_i)=\alpha\times\frac{x_i-\mu_L}{\sqrt{\sigma^2_L+\epsilon}}+\beta</script><h2 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h2><h3 id="Padding-Mask"><a href="#Padding-Mask" class="headerlink" title="Padding Mask"></a>Padding Mask</h3><p>每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。</p>
<h3 id="Sequence-Mask"><a href="#Sequence-Mask" class="headerlink" title="Sequence Mask"></a>Sequence Mask</h3><p>序列在解码的过程中，对于$i$时刻的解码，只能依赖到$i$之前时刻的词语信息，以保证解码过程中的有序性。因此要对$i$时刻后的信息进行masking操作。关于masking的具体做法，主要是设置一个与序列长度相同维度的mask向量，将其第$i$个分量以后的分量全部置为0，第$i$个分量以前的分量全部置为1。用于标记解码序列哪些在网络中的计算是有效的。 产生一个上三角矩阵，下三角的值全为0 </p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="NLPimg\transformer5.jpg" alt="transformer5"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-17-Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-17-Attention/" class="post-title-link" itemprop="url">NLP-17-Attention</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:08:43 / 修改时间：10:13:25" itemprop="dateCreated datePublished" datetime="2020-09-16T10:08:43+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><ul>
<li>ELMo输入可以是一个定长或者不定长的序列，但输出一般要求是一个固定长度的序列（单个标签较为常见，即长度为1的序列）</li>
<li>seq2seq输入与输出的序列长度都不固定，比如翻译</li>
</ul>
<p><img src="NLPimg\attention1.jpg" alt="seq2seq1"></p>
<p>Encoder和Decoder一般都是RNN，通常为LSTM或者GRU</p>
<p><strong>Encoder</strong></p>
<p>“欢迎/来/北京”这些词转换成词向量，也就是Embedding，用$E(w_i)$来表示，与上一时刻的隐状态$h_{i-1}$按照时间顺序进行输入，每一个时刻输出一个隐状态$h_i$我们可以用函数$f$表达RNN隐藏层的变换：$h_i=f(E(w_i),h_{i-1})$。假设有t个词，解码器将输出整个输入序列的m维上下文向量$c$，这个$c$就相当于从“欢迎/来/北京”这几个单词中提炼出来的大概意思一样，包含了这句话的含义。 </p>
<p><strong>Decoder</strong></p>
<p>每一时刻的输入为Encoder输出的$c$和Decoder前一时刻解码的输出$s_{i-1}$，还有前一时刻预测的词的向量$E(o_i)$，我们可以用函数$g$表达解码器隐藏层变换：$s_i=g(c,s_{i-1},E(o_{i-1}))$。直到解码解出“_EOS”，标志着解码的结束。 </p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>seq2seq有个潜在的问题就出在编码器产生的源语言上下文向量c上</p>
<ul>
<li>当输入序列很长时，通过循环网络产生的c向量很难表达整句的信息，而是偏向于表达离序列结尾近的信息 </li>
<li>所有的解码都用同一个上下文c向量，就很难表现出源语言词汇的具体贡献情况（ 目标语言词汇与源语言词汇的大致对应关系 ）</li>
</ul>
<p>Attention机制能解决seq2seq的局限性，注意力机制(Attention)其实本质上就是一种权重分配</p>
<p><img src="NLPimg\attention2.jpg" alt="seq2seq2"></p>
<p>在解码的每一个过程（预测每一个词）中调整权重取到不一样的$c$向量，更通俗的讲就是$c$就是包含“欢迎来北京”这句话的意思，翻译到第一个词“welcome”的时候，需要着重去看“欢迎”这个词。 </p>
<p>假设编码器每个隐藏状态为$h_j$，序列长度为T，那么在第$i$个时刻$c$向量的计算方式如下：</p>
<script type="math/tex; mode=display">
c_1=\alpha_{11}h_1+\alpha_{12}h_2+\dots+\alpha_{1T}h_T=\sum_{j=1}^T\alpha_{1j}h_j</script><script type="math/tex; mode=display">
c_i=\sum_{j=1}^T\alpha_{ij}h_j</script><p>将隐状态$s_{i-1}$与源语言的每个隐状态$h_j$逐一比较，计算关联得分，即可作为每个源语言隐状态$h_j$前的权重$a_{ij}$</p>
<script type="math/tex; mode=display">
\alpha_{ij}=softmax(\alpha(s_{i-1},h_j)),j=1,2,\dots,T</script><p>其中$a(\cdot)$为距离刻画函数，可以是：</p>
<script type="math/tex; mode=display">
\alpha(s_{i-1},h_j)=softmax(v_\alpha^T\tanh(W_\alpha s_{i-1}+U_\alpha h_j))</script><h2 id="Attention变体"><a href="#Attention变体" class="headerlink" title="Attention变体"></a>Attention变体</h2><h3 id="按score-function分"><a href="#按score-function分" class="headerlink" title="按score function分"></a>按score function分</h3><p><strong>加代模型</strong></p>
<script type="math/tex; mode=display">
score(s_i,h_j)=v^T\tanh(Ws_i+Uh_j)</script><p><strong>点积模型</strong></p>
<script type="math/tex; mode=display">
score(s_i,h_j)=s^T_ih_j</script><p><strong>缩放点积模型</strong></p>
<script type="math/tex; mode=display">
score(s_i,h_j)=\frac{s^T_ih_j}{\sqrt{d}}</script><p><strong>双线性模型</strong></p>
<script type="math/tex; mode=display">
score(s_i,h_j)=s^T_iWq</script><p>当输入向量的维度$d$比较高，点积模型的值通常有比较大方差，从而导致softmax函数的梯度会比较小。因此，缩放点积模型可以较好地解决这个问题。</p>
<p>双线性模型可以看做是一种泛化的点积模型。假设公式中$W=U^TV$ ，双线性模型可以写为$score(s_i,h_J)=s^T_iU^TVh_j=(Us_i)^T(Vh_j)$，即分别对$s_i$和$h_j$进行线性变换后计算点积。相比点积模型，双线性模型在计算相似度时引入了非对称性。</p>
<h3 id="按alignment-function分"><a href="#按alignment-function分" class="headerlink" title="按alignment function分"></a>按alignment function分</h3><p><strong>Global Attention</strong></p>
<p>global attention 是所有输入向量作为加权集合，使用 softmax 作为 alignment function。</p>
<p><img src="NLPimg\attention3.png" alt="attention3"></p>
<p><strong>Local Attention</strong></p>
<p>每一次生成目标单词（target word）都分析source sentence中所有单词的做法代价太高。为了缓解这个问题，建议只集中关注一个固定大小2D+1窗口中的source sentence的注释（annotation），即只关注最能帮助预测下一个词汇的source sentence中某个词汇前后step的隐藏状态（hidden state）</p>
<p><img src="NLPimg\attention4.png" alt="attention4"></p>
<p>公式如下：</p>
<script type="math/tex; mode=display">
c_i=\sum^{p_i+D}_{j=p_i-D}\alpha_{i,j}h_j</script><script type="math/tex; mode=display">
p_i=n\cdot\sigma(v^T_p\cdot\sigma(W_ps_{i-1}))</script><script type="math/tex; mode=display">
\alpha_{i,j}=softmax(score(s_{i-1},h_j))\exp(-\frac{(j-p_i)^2}{2(D/2)^2})</script><p>其中，D由用户自定义，而窗口中心即聚焦的$p_i$可以也设为$i$或者基于Decoder之前的隐藏信息$s_{i-1}$决定。$n$表示source sentence的长度。</p>
<p>添加的高斯分布项使得alignment权重随着$j$在窗口中远离中心$p_i$移动逐渐降低，也就是说对于给予靠近$p_i$的annotation更多的影响力。 </p>
<p>另外和global attention不同的是$\alpha_i$的大小被固定为$2D−1$，只有处于窗口范围内的annotation才对输出有影响。</p>
<p>local attention可以看成是alignment权重与一个截断高斯分布相乘后的global attention（窗口外的annotation取值为0）</p>
<h3 id="按generate-context-vector-function分"><a href="#按generate-context-vector-function分" class="headerlink" title="按generate context vector function分"></a>按generate context vector function分</h3><p><strong>Hard Attention</strong></p>
<p>hard attention 是一个随机采样，采样集合是输入向量的集合，采样的概率分布是alignment function 产出的 attention weight。因此，hard attention 的输出是某一个特定的输入向量。 </p>
<p><strong>Soft Attention</strong></p>
<p>soft attention 是一个带权求和的过程，求和集合是输入向量的集合，对应权重是 alignment function 产出的 attention weight。hard / soft attention 中，soft attention 是更常用的，因为它可导，可直接嵌入到模型中进行训练，hard attention 文中 suggests a Monte Carlo based sampling approximation of gradient。 </p>
<h3 id="其他Attention"><a href="#其他Attention" class="headerlink" title="其他Attention"></a>其他Attention</h3><p><strong>Self-Attention</strong></p>
<p>self-Attention(自注意力机制) ，优点是所有待处理的向量在矩阵运算下可以很好的并行处理，加快了以前Encoder和Decoder的效率。</p>
<p>详情见Transformer</p>
<p><strong>Hierarchical Attention</strong></p>
<p>网络可以被看作为两部分，第一部分为词“注意”部分，另一部分为句“注意”部分。整个网络通过将一个句子分割为几部分，对于每部分，都使用双向RNN结合“注意力”机制将小句子映射为一个向量，然后对于映射得到的一组序列向量，我们再通过一层双向RNN结合“注意力”机制实现对文本的分类。</p>
<p><img src="NLPimg\attention5.png" alt="attention5"></p>
<p> RNN 对序列建模，但是缺乏层次信息。而语言本身是具有层次结构，短语组成句子，句子组成篇章。因此研究者十分希望把语言中的这些层次结构在模型中得以体现，Hierarchical 的方式就出现了：从 word attention 到 sentence attention。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-16-ULMFit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-16-ULMFit/" class="post-title-link" itemprop="url">NLP-16-ULMFit</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:07:20 / 修改时间：10:13:16" itemprop="dateCreated datePublished" datetime="2020-09-16T10:07:20+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p>ULMFit（Universal Language Model Fine-tuning）一共是分为3个阶段</p>
<ul>
<li>首先是语言模型的预训练</li>
<li>然后是语言模型的finetune</li>
<li><p>最后是分类任务的finetune</p>
<p><img src="NLPimg\ulmfit1.png" alt="ulmfit1"></p>
</li>
</ul>
<h2 id="通用域语言模型pretrain"><a href="#通用域语言模型pretrain" class="headerlink" title="通用域语言模型pretrain"></a>通用域语言模型pretrain</h2><p>用了一个外部大数据（Wikitext-103，103 million词），先对LM进行pretrain</p>
<h2 id="目标域语言模型fineutune"><a href="#目标域语言模型fineutune" class="headerlink" title="目标域语言模型fineutune"></a>目标域语言模型fineutune</h2><p>通用域的语言模型数据会与目标域的数据有分布上的差别，所以要用目标域的语言数据先把LM finetune一波</p>
<p>用到2个trick：</p>
<p><strong>discriminatice fine-tuning（Discr）</strong></p>
<p>由于底层特征更具有通用性，而顶层特征更具有特殊性，所以作者在训练过程中，对于不同层设置了不同的学习率（底层设置相对较小的学习率，顶层设置相对较大的学习率）</p>
<p>对于最后一层可先设置$n^L$作为学习率，然后只训练最后一层，然后前面的层用$n^{l-1}=n^l/2.6$继续训练。</p>
<p><strong>slanted triangular learning rates（STLR）</strong></p>
<p>学习率先增后减。先用较小的学习率，得到一个好的优化方向，再用较大的学习率进行优化，在训练后期再使用较小的学习率进行更细致的优化。所以类似三角的方式：</p>
<p><img src="NLPimg\ulmfit2.png" alt="ulmfit2"></p>
<script type="math/tex; mode=display">
cut=T\cdot cut\_frac</script><script type="math/tex; mode=display">
p=\begin{cases}
t/cut&t<cut\\
1-\frac{t-cut}{cut\cdot(1/cut\_frac-1)}&otherwise
\end{cases}</script><script type="math/tex; mode=display">
\eta_t=\eta_{max}\cdot\frac{1+p\cdot(ratio-1)}{ratio}</script><p>其中，$cut$表示中间那个尖对应的iteration步数，$T$表示总的迭代步数，$T$表示总的迭代步数，$ratio$表示一个比例参数，$\eta_{max}$是最大的学习率（尖对应的纵坐标）。一般取$cut_frac=0.1,ratio=32,\eta_{max}=0.01$</p>
<h2 id="分类任务finetune"><a href="#分类任务finetune" class="headerlink" title="分类任务finetune"></a>分类任务finetune</h2><p>加入两个全连接模块（带BN和ReLU激活的），进行分类即可。</p>
<p>用到4个trick：</p>
<p><strong>concat pooling</strong></p>
<p>如果仅使用 RNN 模型最后一个 time step 的输出，显然会丢失信息，尤其是在长文本建模中，因此作者对 RNN 所有 time state 的 hidden states 进行 max pooling 和 mean pooling，然后将 pooling 得到的两个特征与最后一个 time step 的输出连接，作为最终输出。</p>
<script type="math/tex; mode=display">
h_c=[h_T,maxpool(H),meanpool(H)]</script><p><strong>gradual unfreezing</strong></p>
<p>直接fine-tuning整个网络可能导致网络遗忘之前预训练得到的通用特征。具体做法是自顶向下以epoch为单位逐步进行fine-tuning，即第一个epoch只解冻最后一层，第二个epoch解冻最后两层，以此类推。</p>
<p><strong>BPT3C</strong></p>
<p>为了使大型文档的分类器微调可行，作者将文档划分为大小为 b 的固定长度批次。 在每个批次的开头，用前一批次的最终状态初始化模型，跟踪平均值和最大池的隐藏状态，梯度反向传播到批次。</p>
<p><strong>双向语言模型</strong></p>
<p>分别训练了前向和后向的 Language Model，在 fine-tuning 阶段对预测的结果取平均。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-15-ELMo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-15-ELMo/" class="post-title-link" itemprop="url">NLP-15-ELMo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:06:15 / 修改时间：10:13:08" itemprop="dateCreated datePublished" datetime="2020-09-16T10:06:15+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>Word2Vec每个词对应一个固定的向量，对于多义词无能为力</li>
<li>ELMo会根据上下文来推断每个词对应的词向量，对于多义词，可以结合前后语境对多义词进行理解</li>
</ul>
<h2 id="LSTM语言模型流程"><a href="#LSTM语言模型流程" class="headerlink" title="LSTM语言模型流程"></a>LSTM语言模型流程</h2><p>给定一串长度为$N$的词条$(t_1,t_2,\dots,t_N)$，前向语言模型通过对给定历史$(t_1,t_2,\dots,t_{k−1})$预测$t_k$进行建模</p>
<p><img src="NLPimg\elmo4.png" alt="elmo4"></p>
<p>模型公式：</p>
<script type="math/tex; mode=display">
p(t_1,t_2,\dots,t_N)=\prod^N_{k=1}p(t_k|t_1,t_2,\dots,t_{k-1})</script><p><strong>流程</strong></p>
<p>以“the cat sat on the mat”这句话为例。在某一个时刻$k$时，输入为the，输出cat的概率。</p>
<ol>
<li>将the转换成word embedding，对应输入$x_k$，其中$x_k$为$n\times1$维列向量</li>
<li>将上一时刻的输出/隐状态$h_{k−1}$及第一步中的$x_k$一并送入lstm，并得到输出及隐状态$h_k$，其中$h_{k-1}$和$h_k$是$m\times1$维列向量</li>
<li>将lstm的输出$h_k$，与上下文矩阵$W’$相乘，即$W’h_k$得到一个列向量，再将该列向量经过softmax归一化。其中，假定数据集有$V$个单词，$W’$是$|V|\times m$的矩阵，$h_k$是$m\times1$的列向量，于是最终结果是$|V|\times1$的归一化后向量，即从输入单词得到的针对每个单词的概率。 </li>
</ol>
<h2 id="ELMo的biLSTM语言模型流程"><a href="#ELMo的biLSTM语言模型流程" class="headerlink" title="ELMo的biLSTM语言模型流程"></a>ELMo的biLSTM语言模型流程</h2><p>相对于LSTM有两个改进，第一个是使用了多层LSTM，第二个是增加了后向语言模型</p>
<p>对于英语文本，可以在输入层和输出层加入CNN结构，减少规模参数并解决OOV（Out-of-Vocabulary）问题</p>
<p><img src="NLPimg\elmo5.jpg" alt="elmo5"></p>
<p>后向LSTM模型公式：</p>
<script type="math/tex; mode=display">
p(t_1,t_2,\dots,t_N)=\prod^N_{k=1}p(t_k|t_{k+1},t_{k+2},\dots,t_N)</script><p>ELMo将前后向结合，得出最大似然公式：</p>
<script type="math/tex; mode=display">
L=\sum_{k=1}^N(\log p(t_k|t_1,t_2,\dots,t_{k-1};\Theta_x,\overrightarrow{\Theta_{LSTM}},\Theta_s)
+p(t_k|t_{k+1},t_{k+2},\dots,t_N;\Theta_x,\overleftarrow{\Theta_{LSTM}},\Theta_s))</script><p>其中，$\overrightarrow{\Theta_{LSTM}}$表示前向lstm的网络参数，反向的lstm的网络参数同理，$\Theta_x$表示映射层的共享（word embedding），$\Theta_s$表示上下文矩阵的参数（概率计算）</p>
<h2 id="表征生成"><a href="#表征生成" class="headerlink" title="表征生成"></a>表征生成</h2><p>假定前向lstm语言模型的第$j$层第$k$时刻的输出向量为$\overrightarrow{h^{LM}_{k,j}}$，后向lstm的第$j$层的第$k$时刻的输出向量为$\overleftarrow{h^{LM}_{k,j}}$</p>
<p>对于每个单词（token）$t_k$，对于L层的双向lstm语言模型，一共有2L+1个表征</p>
<script type="math/tex; mode=display">
R_k=\{x_k^{LM},\overrightarrow{h^{LM}_{k,j}},\overleftarrow{h^{LM}_{k,j}}|j=1,2,\dots,L\}=\{h^{LM}_{k,j}|j=0,1,\dots,L\}</script><p>其中，$h^{LM}_{k,0}$是word embedding，也就是lstm的输入。对于每一层的双向lstm语言模型，$h^{LM}_{k,j}=[\overrightarrow{h^{LM}_{k,j}};\overleftarrow{h^{LM}_{k,j}}]$</p>
<p>ELMo向量的生成可以是最顶层lstm的输出，即$h^{LM}_{k,L}$，或者对每层向量加权重$s^{task}_j$，将每层的向量与权重相乘，然后再乘以一个权重$\gamma$</p>
<script type="math/tex; mode=display">
ELMO^{task}_k=E(R_k;\Theta^{task})=\gamma^{task}\sum^L_{j=0}s^{task}_jh^{LM}_{k,j}</script><p>一个单词的最终向量是$[x_k;ELMo^{task}_k]$。</p>
<p>经过实验，高层用于语义理解或词义消歧效果好；低层对于句法信息或词性标注效果好</p>
<ul>
<li>添加$s^{task}_j$是因为每层学的东西不一样，侧重点不同</li>
<li>添加$\gamma$是因为需要配合最终向量$x_k$的缩放系数</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/16/NLP-14-%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sholic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHolic的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/09/16/NLP-14-%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">NLP-14-记忆网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-09-16 10:04:56 / 修改时间：10:13:03" itemprop="dateCreated datePublished" datetime="2020-09-16T10:04:56+08:00">2020-09-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="RNN结构"><a href="#RNN结构" class="headerlink" title="RNN结构"></a>RNN结构</h2><p>RNN是有时序的，每个时序里，都是单元的重复 </p>
<p><img src="NLPimg\memory1.jpg" alt="elmo1"></p>
<p>在第$t$时刻时，假定输入为$x_t$，隐状态向量为$h_{t−1}$，则下一隐状态$h_t$则由下式产生</p>
<script type="math/tex; mode=display">
h_t=\tanh(Wx_t+Uh_{t-1}+b)</script><p>其中，$\tanh$将信息整理到区间(-1,1)，生成候选信息。</p>
<p>$x_t$是$n\times1$的列向量，$h_t$是$m\times1$的列向量。矩阵$W$的维数是$m\times n$，矩阵$U$的维数是$m\times m$，$b$是$m\times1$的列向量。参数个数$mn+mm+m=m(n+m+1)$</p>
<h2 id="LSTM结构"><a href="#LSTM结构" class="headerlink" title="LSTM结构"></a>LSTM结构</h2><p>LSTM比RNN多了遗忘门，解决长时依赖问题和梯度消失问题</p>
<p><img src="NLPimg\memory2.png" alt="elmo2"></p>
<ul>
<li>输入门$i_t$，控制保留多少当前记忆</li>
<li>遗忘门$f_t$，控制保留多少之前记忆</li>
<li>规范后的信息$g_t$</li>
<li>输出门$o_t$，控制保留多少单元状态</li>
<li>单元状态$c_t$</li>
<li>隐藏状态$h_t$</li>
</ul>
<p>各种门代表各种信息的筛选比例，单元状态$c_t$是<strong>筛选过的当前信息</strong>和<strong>筛选过的过去信息</strong>的加权平均</p>
<p>公式如下：</p>
<script type="math/tex; mode=display">
i_t=\sigma(W^ix_t+U^ih_{t-1}+b^i)</script><script type="math/tex; mode=display">
f_t=\sigma(W^fx_t+U^fh_{t-1}+b^f)</script><script type="math/tex; mode=display">
o_t=\sigma(W^ox_t+U^oh_{t-1}+b^o)</script><script type="math/tex; mode=display">
g_t=\tanh(W^gx_t+U^gh_{t-1}+b^g)</script><script type="math/tex; mode=display">
c_t=f_t\odot c_{t-1}+i_t\odot g_t</script><script type="math/tex; mode=display">
h_t=o_t\odot\tanh(c_t)</script><p>其中，$\tanh$将信息整理到区间(-1,1)，生成候选信息；$\sigma$将信息整理到区间(0,1)，表示允许信息通过的多少。</p>
<p>输入依然为$x_t$，维度为$n\times1$的列向量。输出及隐状态$h_t$是$m\times1$的列向量。 $c_t$是携带信息的列向量，向量维度是$m\times1$。$W$系列的维度与RNN的$W$一致，均为$m\times n$。$U$与RNN的$U$一致，均为$m\times m$。$b$系列的维度是$m\times1$的列向量。参数的总个数是$4m(n+m+1)$</p>
<h2 id="GRU结构"><a href="#GRU结构" class="headerlink" title="GRU结构"></a>GRU结构</h2><p>是LSTM的变体，只有重置门和更新门。</p>
<p><img src="NLPimg\memory3.png" alt="memory3"></p>
<ul>
<li>重置门$r_t$，控制保留多少之前记忆</li>
<li>更新门$z_t$，控制保留多少当前记忆</li>
<li>规范且重置后的隐藏信息$\tilde h_t$</li>
<li>隐藏信息$h_t$</li>
</ul>
<p>各种门代表各种信息的筛选比例，隐藏状态$h_t$是<strong>筛选过的当前信息</strong>和<strong>筛选过的过去信息</strong>的加权平均</p>
<p>公式如下：</p>
<script type="math/tex; mode=display">
z_t=\sigma(W^zx_t+U^zh_{t-1}+b^z)</script><script type="math/tex; mode=display">
r_t=\sigma(W^rx_t+U^rh_{t-1}+b^z)</script><script type="math/tex; mode=display">
\tilde h_t=\tanh(W^hx_t+U^h(r_t*h_{t-1})+b^h)</script><script type="math/tex; mode=display">
h_T=(1-z_t)*h_{t-1}+z_t*\tilde h_t</script><p>其中，$\tanh$将信息整理到区间(-1,1)，生成候选信息；$\sigma$将信息整理到区间(0,1)，表示允许信息通过的多少。</p>
<p>如果$r_t$为1，而$z_t$为0 那么相当于变成了一个标准的RNN，能处理短距离依赖</p>
<p>输入依然为$x_t$，维度为$n\times1$的列向量。输出及隐状态$h_t$是$m\times1$的列向量。$W$系列的维度与RNN的$W$一致，均为$m\times n$。$U$与RNN的$U$一致，均为$m\times m$。$b$系列的维度是$m\times1$的列向量。参数的总个数是$3m(n+m+1)$</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sholic</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sholic</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
